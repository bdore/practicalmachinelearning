---
title: "Practical Machine Learning Course Project"
author: "Bernardo Dore"
date: "13th June 2016"
output: html_document
---
#Synopsis

This project deals with the application of a machine learning technique to make predictions on the field of Human Activity Recognition.

The data from the project is available from:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data comes from a study as follows:

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

More details in the paper "Qualitative Activity Recognition of Weight Lifting Exercises" released by the authors of the study.

http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201

##Feature selection
A machine learning model "learns" from a set of features (or variables) that produce an outcome.
In order to develop a model that is both efficient and effective, the right set of features must
be determined. Too many features may have a big computational cost and not contibute to the overall performance of the model. Too few and the model may not be able to find a good fit to predict the outcomes on new observations.

Exploring the datasets it is possible to see that many variables in the test set have only NA values.

The idea is to use all variables in the test set data have some data in them, in order to build the simplest possible model and keep the meaningful variables.

A variable with no values would have zero impact on predictions made on the test set.
We will identify those variables in the test set and then remove the same variables from the training set.

Some variables do not seem to be meaningful, like the row index, user name, timestamps and
the flag of a new window or not.

The timestamps are because the actual time the user perfomed the actions is of no importance for predictions since a new user may perform the action at any time in the future. The underlying  structure of the timeseries is still preserved in the data generated by the sensor readings.
```{r, message=FALSE}
library(caret)
set.seed(3243)
```

```{r}
training <- read.csv("pml-training.csv", na.strings = c("", "#DIV/0!"))
testing <- read.csv("pml-testing.csv")
``` 

```{r}
#We remove the first index column, the user name, timestamps and new window from both sets.

training <- training[,-c(1:6)]
testing <- testing[,-c(1:6)]
```


```{r}
#This function checks, for each variable in the dataset, if the sum of NA values is equal to the length of the variable.

removeNa <- function(dataframe){
  varIndex <- c()
  for (i in 1:length(names(dataframe))){
    if(sum(is.na(dataframe[,i])) == length(dataframe[,i])){
      varIndex <- c(varIndex, i)
    }
  }
  dataframe <- dataframe[-varIndex]
}
```

Now we apply the function to the test set to remove the variables.
```{r}
testing <- removeNa(testing)
dim(testing)
```
The test set now is reduced to 54 variables.

The test set contains a variable "problem_id" that is not present in the training set. All the other variables are removed while keeping the "classe" variable which is our response variable.
```{r}
#take all variable names left in the test set, except for "problem_id".
varNames <- names(testing[1:53])

#removes variables list from the training set while keeping the response variable.
training <- training[,c(varNames, "classe")]
```

#Data preprocessing
In order to compensate for different ranges in the variable values we standardize both sets. This sets each value to a number which represents standard deviations removed from the mean of the variable.

```{r}
#remove "problem_id" variable from test set
testing <- testing[1:53]
testing[,1:53] <- scale(testing[,1:53])

training[,1:53] <- scale(training[,1:53])
```

#Choosing and fitting a model
As described in the paper, the authors chose the random forest algorithm to perform the classification. I decided to use the same.

#Cross-validation and out of sample errors
Random forests are a somewhat special case when it comes to cross-validation. According to these sources[1][2][3]:

[1]Random Forests,
Leo Breiman and Adele Cutler
https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr

[2]Breiman, Leo (2001). Random forests. pp. 8
https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf

[3]Gareth James; Daniela Witten; Trevor Hastie; Robert Tibshirani (2013). An Introduction to Statistical Learning. Springer. pp. 317-318.
http://www-bcf.usc.edu/~gareth/ISL/

[1]"**In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error**. It is estimated internally, during the run, as follows:

Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree.

Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests."

[2]"Tibshirani [1996] and Wolpert and Macready [1996], proposed using out-of-bag estimates as an ingredient in estimates of generalization error. Wolpert and Macready worked on regression type problems and proposed a number of methods for estimating the generalization error of bagged predictors. Tibshirani used out-of-bag estimates of variance to estimate generalization error for arbitrary classifiers. The study of error estimates for bagged classifiers in Breiman [1996b], gives empirical evidence to show that the out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set."

[3]"It turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. (...)
The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation."

So, in practice, the algorithm performs bootstrapping effectively creating new samples as it runs. The out-of-bag error estimation is the equivalent of an out-of-sample error estimation.

The out-of-bag method "oob" set in trainControl() is the out-of-sample error estimate for Random Forests.

```{r}
myTrControl <- trainControl(method = "oob")
```

Fit model using random forests.
```{r, message=FALSE}
model <- train(classe ~., training, method="rf", trControl = myTrControl)
```

```{r}
print(model)
```

The accuracy is **`r model$results[2,1]`.**

```{r}
print(model$finalModel)
```

The error rate estimation is **0.11%.**